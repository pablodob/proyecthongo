{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project_hongo\n",
    "\n",
    "## Introducción\n",
    "\n",
    "### 1- Trabajar los datos\n",
    "transformar los datos de ingreso, a variables de ingreso.\n",
    "\n",
    "### 2- Diseño de la red\n",
    "Pensar en la estructura.\n",
    "\n",
    "### 3- Costo\n",
    "Definir la función de costo para entrenar la red.\n",
    "\n",
    "### 4- Definir el entrenamiento\n",
    "Manera a modificar los parametros (pesos)\n",
    "\n",
    "### 5- Prueba\n",
    "entrenamiento y testeo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mushrooms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "planedata = []\n",
    "all_possible_values = []\n",
    "planedata=np.reshape(np.array(planedata),(len(data),0))\n",
    "\n",
    "num_values = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(8124, 0), dtype=float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data in planedata variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for atr in data.columns:\n",
    "\n",
    "    counter = Counter(data[atr])\n",
    "    parcial_matrix = np.zeros((len(data),len(counter)))\n",
    "\n",
    "    possible_values_parcial = list(counter.keys())\n",
    "    all_possible_values += possible_values_parcial\n",
    "\n",
    "    values = list(data[atr])\n",
    "\n",
    "    n_reg=0\n",
    "    for reg in values:\n",
    "        index = possible_values_parcial.index(reg)\n",
    "        parcial_matrix[n_reg,index] = 1\n",
    "        n_reg += 1\n",
    "\n",
    "    planedata = np.append(planedata,parcial_matrix,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_data=planedata[:,0:1]\n",
    "inputs_data=planedata[:,2:len(planedata[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8124, 117)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(inputs_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0.01\n",
    "epoch = 30\n",
    "batch_size = 20\n",
    "n_train = int(len(labels_data)*.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide data in inputs and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train=labels_data[0:n_train]\n",
    "labels_test=labels_data[n_train+1:len(labels_data)]\n",
    "\n",
    "inputs_train=inputs_data[0:n_train]\n",
    "inputs_test=inputs_data[n_train+1:len(inputs_data),:]\n",
    "\n",
    "list_labels_train = []\n",
    "list_inputs_train = []\n",
    "\n",
    "num_data = int((len(labels_train)/batch_size))\n",
    "\n",
    "for i in range (num_data):\n",
    "    list_inputs_train.append(inputs_train[i*batch_size:(i+1)*batch_size])\n",
    "    list_labels_train.append(labels_train[i*batch_size:(i+1)*batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializater Tensor Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "inputs = tf.placeholder(shape=(None,len(inputs_train[0])), dtype=tf.float32, name=\"inputs\")\n",
    "\n",
    "# Labels\n",
    "labels = tf.placeholder(shape=(None,1), dtype=tf.float32, name=\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "input_layer = tf.contrib.layers.fully_connected(inputs=inputs, num_outputs = 25, activation_fn = tf.nn.relu)\n",
    "hidden_layer = tf.contrib.layers.fully_connected(inputs=input_layer, num_outputs = 5, activation_fn = tf.nn.relu)\n",
    "outputs = tf.contrib.layers.fully_connected(inputs=hidden_layer, num_outputs = 1, activation_fn = tf.nn.sigmoid)\n",
    "\n",
    "# Initializater TensorFlow\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Cost_and_Optimizer\"):\n",
    "    cost = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=outputs, name=\"cost\")) / batch_size\n",
    "    optimizer = tf.train.GradientDescentOptimizer(l)\n",
    "    train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch N°: 0 - Cost = 253.0043182373047 - Accuracy = 52.633018738886605%\n",
      "Train: Epoch N°: 10 - Cost = 214.8970947265625 - Accuracy = 90.13814799617016%\n",
      "Train: Epoch N°: 20 - Cost = 201.1691436767578 - Accuracy = 98.30392559157434%\n",
      "Test: Cost = 20.48482322692871 - Accuracy = 99.26108374384236%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./summaries/lamnda {} epoch {} batch {}\".format(l,epoch,batch_size))\n",
    "    writer.add_graph(sess.graph)\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(epoch):\n",
    "        for j in range(num_data):\n",
    "            sess.run(train, feed_dict={inputs: list_inputs_train[j], labels: list_labels_train[j]})\n",
    "\n",
    "        #To calcule acurracy and cost\n",
    "        if i%10==0:\n",
    "            partial_cost = sess.run(cost, feed_dict={inputs: inputs_train, labels: labels_train})\n",
    "            result = sess.run(outputs, feed_dict={inputs: inputs_train, labels: labels_train})\n",
    "            predictions = result > 0.5\n",
    "\n",
    "            accuracy = np.sum(labels_train == predictions) / len(labels_train)\n",
    "\n",
    "            print(\"Train: Epoch N°: {} - Cost = {} - Accuracy = {}%\".format(i, partial_cost, accuracy * 100))\n",
    "\n",
    "#    s = sess.run(merged_summary, feed_dict={inputs: inputs_train, labels: labels_train})\n",
    "#    writer.add_summary(s, i)\n",
    "\n",
    "    # Print cost test and final acurracy\n",
    "    final_cost = sess.run(cost, feed_dict={inputs: inputs_test, labels: labels_test})\n",
    "    outputs = sess.run(outputs, feed_dict={inputs: inputs_test, labels: labels_test})\n",
    "    predictions = outputs > 0.5\n",
    "\n",
    "    final_accuracy = np.sum(labels_test == predictions) / len(labels_test)\n",
    "\n",
    "print(\"Test: Cost = {} - Accuracy = {}%\".format(final_cost, final_accuracy* 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
